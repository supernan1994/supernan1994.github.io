---
layout: post
title:  "深入理解WMD算法（一）：Paper Reading"
date:   2018-12-30 00:50:00
categories: nlp
comments: true
image: 2018-12-30-wmd1/cover.png
tags: [nlp]
---

----
* any list
{:toc}

----

## 0. 背景
2018年Q4，我承接了公司级的一个[Key Result](https://en.wikipedia.org/wiki/OKR)：**重构问答机器人的后端架构，实现单机QPS能力提升10倍。** 季度末复盘时，我们在完成QPS提升11倍、TP99下降至原来的1/10的同时，支撑算法部门达成机器人效果提升11%的指标。在问答重构的系统设计与实现过程中我们积累了一些值得参考的经验，因此选择一些有代表性的分享出来。  
本系列主要介绍WMD算法，将分为两篇文章：原理和工程化实现。本篇文章会主要介绍算法的原理。  
^
在深入了解算法之前，我们先简单了解一下**『[吾来机器人平台](https://laiye.com/)』**的问答机器人的业务流程，帮助我们更好的理解WMD在业务中的使用场景。  
问答机器人的数据来源是领域知识库，知识库中包含多个知识点，每个知识点包含多个语义相同的相似问题，这些相似问题由机器和人工泛化而来。并且这些相似问题可以以同样的答案回复。下图是**海淀区科技园『码上办』项目**的知识库中的一个知识点。  
<div align="center"><img width="100%" height="100%" src="2018-12-30-wmd1/knowledge.png"/></div>
^
当用户向机器人提出问题时，问答机器人会在知识库中找到最相似的相似问题，把相似问题对应的知识点的答案回复给用户。
<div align="center"><img width="80%" height="80%" src="2018-12-30-wmd1/qa.png"/></div>
^
下图是问答机器人的架构：
^
<div align="center"><img width="50%" height="50%" src="2018-12-30-wmd1/architect.png"/></div>

1. 检索(Retrieval)
- 将每个相似问作为一个可被检索的文档
- 输出为若干个(query, question)句对
- 每个相似问题都对应一个知识点
2. 匹配(Matching)
- 计算(query, question)的语义相似度特征
    - 无监督特征
    - 有监督特征
3. 排序(Ranking)
- 知识库内的相似问，构造句对训练数据，训练有监督的模型
- 判断(query, question)的分数，选择分数最高的问题对应的知识点作为机器人回复

今天我们讨论的WMD是匹配过程中其中一个无监督的语义相似度特征。我们用WMD计算句对的文本相似度，并将它作为排序模型的一个输入，来影响问答机器人最终的回复结果。

## 1. 概述
WMD（Word Mover's Distance）[^wmd]是2015年提出的一种衡量文本相似度的方法。它具有以下几个优点：

- 效果出色：充分利用了[word2vec](https://en.wikipedia.org/wiki/Word2vec)的领域迁移能力
- 无监督：不依赖标注数据，没有冷启动问题
- 模型简单：仅需要词向量的结果作为输入，没有任何超参数
- 可解释性：将问题转化成线性规划，有全局最优解
- 灵活性：可以人为干预词的重要性

当然它也有明显的缺点：

- 时间复杂度较高：$$O(p^{3}\log{}p)$$（其中，p代表两篇文本分词去重后词表的大小）

## 2. WMD核心算法

### 2.1 形象描述
WMD的核心思想非常简单，就是我们在文章开头看到的那张图。在计算两条文本的相似度时：

- 利用word2vec将词编码成词向量
- 去掉停用词
- 计算出每个词在文本中所占权重，一般用词频来表示
- 对于每个词，找到另一条文本中与之最相近的词，计算词向量之间的距离。用词向量距离与词频相乘得出两个词的转移代价
- 保证全局的转移代价加和是最小的
- 词可以部分或全部的转移到另一个词（下文将举例描述此种情况）

<div align="center"><img width="100%" height="100%" src="2018-12-30-wmd1/cover.png"/></div>

论文中给出了两个例子：
- 第一个例子是分别计算$$D_{1}$$和$$D_{2}$$与$$D_{0}$$的距离，比较哪条文本与$$D_{0}$$相似度更高。$$D_{1}$$有四个词，对于每一个词，都能找到在$$D_{0}$$中与之最相近的一个词，并计算出转移代价。与我们直观的理解相符，$$D_{1}$$与$$D_{0}$$的距离比$$D_{2}$$与$$D_{0}$$的距离更近。
<div align="center"><img width="50%" height="50%" src="2018-12-30-wmd1/wmd_example1.png"/></div>
- 在第二个例子中，两条文本的词数并不一致，$$D_{0}$$中每个词的词频是$$\frac{1}{4}$$，而$$D_{3}$$中每个词的词频是$$\frac{1}{3}$$，因此$$D_{3}$$的每个词将对应到$$D_{0}$$多个词上。
<div align="center"><img width="50%" height="50%" src="2018-12-30-wmd1/wmd_example2.png"/></div>
^
### 2.2 数学描述
WMD算法用以上的思路将文本语义相似度的问题转换成了一个简单的[最小费用最大流问题](https://en.wikipedia.org/wiki/Minimum-cost_flow_problem)。用数学语言形式化的描述WMD所解决的问题：  
^
首先，假设有一个训练好的词向量矩阵$$\textbf{X}\in\mathbb{R}^{d \times n}$$，一共有$$n$$个词。第$$i$$列$$\textbf{x}\in\mathcal{R}^{d}$$代表第$$i$$个词的$$d$$维词向量。词$$i$$与词$$j$$欧氏距离为

$$c(i,j)=\Vert\textbf{x}_{i}-\textbf{x}_{j}\Vert_{2}$$

一条文本可以用稀疏向量$$\textbf{d} \in \mathcal{R}^{n}$$作为词袋表示，如果在文本中词$$i$$出现了$$c_{i}$$次，

$$d_{i}=\frac{c_{i}}{\sum_{j=1}^{n}c_{j}}$$

我们令$$\textbf{d}$$和$$\textbf{d}'$$分别代表要计算的两条文本的词袋表示，$$\textbf{d}$$中的每个词$$i$$都可以全部或部分地转移到$$\textbf{d}'$$。因此，定义一个稀疏的转移矩阵$$\textbf{T}\in\mathcal{R}^{n \times n}$$，$$\textbf{T}_{ij}$$（$$\textbf{T}_{ij}\ge 0$$）代表了有多少从$$\textbf{d}$$中的词$$i$$转移到$$\textbf{d}'$$中的词$$j$$。进一步地，定义从$$\textbf{d}$$到$$\textbf{d}'$$转移代价的和为：$$\sum_{i,j}\textbf{T}_{ij}c(i,j)$$。  
最终我们将最小化转移代价的和转化为线性规划问题：

$$\min_{\textbf{T} \ge 0}\sum_{i,j=1}^n\textbf{T}_{ij}c(i,j)$$

s.t.

$$ \sum_{j=1}^n\textbf{T}_{ij}=d_{i} \qquad \forall i \in {1,...,n}$$

$$ \sum_{i=1}^n\textbf{T}_{ij}=d'_{j} \qquad \forall j \in {1,...,n}$$

### 2.3 时间复杂度
上文的线性规划的解法论文中并没有给出描述，因为在计算机视觉学科已经有类似的研究成果了，也就是EMD（Earth Mover’s Distance）[^fastemd]，WMD算法也是来源于EMD的思路。  
这里我们对具体解法不做详细介绍，只给出几种常见解法的时间复杂度：
- [单纯形法](https://en.wikipedia.org/wiki/Simplex_algorithm)：指数级时间复杂度
- [内点法](https://en.wikipedia.org/wiki/Interior-point_method)：多项式级时间复杂度
- Fast EMD：$$O(p^{3}\log{}p)$$（其中，p代表两篇文本分词去重后词表的大小）

更多细节可以参考EMD[^emd]和Fast EMD[^fastemd]的论文对时间复杂度的解释。

## 3. 算法速度改进
### 3.1 WCD（Word centroid distance）
### 3.2 RWMD（Relaxed word moving distance）
### 3.3 Prefetch and prune

## 4. 开源实现

## 5. 总结

## 参考文献
[^wmd]: http://proceedings.mlr.press/v37/kusnerb15.pdf
[^emd]: https://www.cs.cmu.edu/~efros/courses/AP06/Papers/rubner-jcviu-00.pdf
[^fastemd]: http://www.cs.huji.ac.il/~werman/Papers/ICCV2009.pdf